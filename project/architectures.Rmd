---
title: Architectures
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Multiple types of neural network architectures have been proposed. Some examples
are:

1. Recurrent neural networks - networks that have recurrent connections
between nodes;
1. Convolution neural networks - networks that replace the dot product with a
convolution operation that summarizes multiple elements from an input, e.g
pixels;
1. Capsule networks - use capsules, which are collections of nodes, instead of
independent network nodes;
1. [Others](https://www.asimovinstitute.org/neural-network-zoo/).

The group decided to use a simple feed-forward neural network, which uses nodes
that are fully connected between layers. The decision to use a simple network
is two-fold:
1. Simplicity - although the model itself can be come complex due to the high
number of parameters, the group does not have to be concerned with models that
have very complex architectures;
2. Ease of implementation - a neural network that follows a simple architecture
is typically easier to implement by comparison to networks that have skip
connections (node connections that skip layers), recurrent relations (a node's
output is placed back into the node), etc.


As mentioned, the group decided to use a feed-forward neural network. The
network was configured to flatten the $28 \times 28$ to a vector of 784 values.
The second and third layer were configured to use 512 nodes. Finally, the output
of the network was structured to use 10 nodes as the number of classes in the
MNIST Fashion dataset is 10. The following represents a summary of the network
architecture:

| Layer | Size       |
|-------|------------|
| 0     | (784, 1)   |
| 1     | (784, 512) |
| 2     | (512, 10)  |
| 3     | (10, 1)    |


![](./ANN.jpg)
