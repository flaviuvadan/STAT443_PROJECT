---
title: Linear Algebra
output: 
    html_document:
        css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The input to the network is an initial vector of predictor variables ($\vec{x}$)
. The main operation that passes the input through the network is a simple dot
product between a layer's node output values and the weights of the edges
between the nodes of layer $i$ and layer $i+1$. Each element that is an input to
layer $i$ is a linear combination of all the outputs of layer $i-1$ and the
weights between $i$ and $i-1$. The weights are the parameters the network is
trying to learn.

As an example, the input layer represents an $m \times 1$ dimensional vector
($\vec{l}_1$). In order to create the activations of the next layer, the network
needs to take the dot product of $\vec{l}_1$ and the matrix of weights
$\textbf{W}$:

$$ \vec{x}^T \textbf{W} = \vec{a}_1^T $$

Where,
$$ \textbf{W} = [\vec{w}_1, \vec{w}_2, ... , \vec{w}_m] $$

So, when looking at the operation for the first element of $\vec{a}_1^T$, the
result is simply the dot product of $\vec{x}$ and $\vec{w}_1$ (the first vector
of weights).

This process is then repeated until the input is reduced to a vector that is
$k \times 1$ (one element for each group that we wish to classify each
  observation into).

At each layer, the output of a node is taken after its input passed through an
activation function. Activation functions (also known as transfer functions) are
used in neural networks primarily to sort the final result in a manner that can
bring comparisons between classes into a clear solution. We can think of an
activation function as determining whether a signal gets propagated through a
network or not and to what magnitude is it propagated. Some of the activations
used in the group's implementation are:

1. **Rectified linear units** (ReLU): $R(z) = max(0, z)$;
1. **Sigmoid**: $S(x) = \frac{1}{1+e^{-x}}$

The final layer of the neural network uses a softmax activation function to
create an output distribution over the classes the network attempts to classify.

$$ \sigma(z)_j = \frac{e^{z_j}}{\sum_{i=1}^k e^{z_i}} for j = 1, ..., k$$

The **softmax** takes each activation from the previous layer and divides it by the
summed total of activations, which creates a value in the range [0, 1] for each
class. The predicted class label corresponds to the highest value in the output
distribution of the network e.g if index 1 in the SoftMax output contains the
highest value, the prediction will be 1, which corresponds to a specific class.

The reasoning behind using the softmax function is because it is **differentiable**.
Given the function is differentiable, the network can attempt to minimize the
difference between the empirical distribution, which is defined by the training
dataset, and the model distribution, which is defined by its output. For more
details about minimizing the difference between the output distribution and the
model distribution, read about [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
divergence!
